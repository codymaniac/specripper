
# Guide: Building a Level 4 Semantic Python Pipeline

This document provides a complete, step-by-step guide to building a Python-based processing pipeline. This pipeline will take the structured JSON output from the **SpecRipper** application and perform advanced semantic analysis, such as finding redundant requirements using AI sentence embeddings.

This is the "Level 4" system we've discussed. It runs on your local machine (or a server) and is separate from the Next.js web application.

---

## 1. Core Purpose & Vision

The goal of this Python pipeline is to add deep semantic understanding to the structurally excellent data prepared by SpecRipper. While SpecRipper handles the cleaning and rule-based tagging, this pipeline will use AI models to understand the *meaning* of the text.

**What you will build:**
A Python script that can be run from your command line to:
1.  Load the `chunks.json` file generated by SpecRipper.
2.  Use a powerful AI model to generate a "sentence embedding" (a vector of numbers representing the meaning) for each chunk.
3.  Use these embeddings to calculate the semantic similarity between all chunks.
4.  Identify and flag chunks that are likely duplicates or highly redundant.
5.  Save a new, enriched JSON file with this similarity information.

**This is the second major step of your project**, to be performed *after* you have used the SpecRipper application to get your `chunks.json` file.

## 2. Setting Up Your Python Environment

Before writing code, you need a clean and isolated Python environment. This can be done on your Windows PC or on another machine that can have Python and internet access (for the initial model download).

### Step 1: Install Python
If you don't have Python installed, download and install it from [python.org](https://www.python.org/downloads/). Version 3.9 or higher is recommended.

### Step 2: Create a Project Folder
Create a new folder on your computer for this pipeline. For example, `spec-pipeline`.

### Step 3: Create a Virtual Environment
A virtual environment ensures that the packages you install for this project don't interfere with other Python projects on your system.

1.  Open your terminal or command prompt.
2.  Navigate into your new project folder:
    ```bash
    cd spec-pipeline
    ```
3.  Create the virtual environment:
    ```bash
    python -m venv venv
    ```

### Step 4: Activate the Virtual Environment
You must activate the environment each time you work on the project.

*   **On macOS and Linux:**
    ```bash
    source venv/bin/activate
    ```
*   **On Windows:**
    ```bash
    .\venv\Scripts\activate
    ```

Your terminal prompt should now show `(venv)` at the beginning, indicating the environment is active.

### Step 5: Install Necessary Libraries
With your environment active, install the required Python libraries using `pip`. We need:
- `sentence-transformers`: For generating the AI sentence embeddings.
- `scikit-learn`: For calculating similarity.
- `pandas`: For easily handling the data.

Run the following command:
```bash
pip install sentence-transformers scikit-learn pandas
```

Your environment is now ready!

## 3. The Python Script

Create a new file in your `spec-pipeline` folder named `process.py`. Copy and paste the following code into it. Comments in the code explain what each part does.

```python
# process.py

import json
from sentence_transformers import SentenceTransformer, util
import torch
import pandas as pd

# 1. LOAD THE MODEL
# This loads a powerful, pre-trained AI model for understanding text.
# The first time you run this, it will download the model (requires internet).
# After that, it will be cached and work completely offline.
print("Loading sentence-transformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2')
print("Model loaded.")

# 2. LOAD THE DATA FROM SPECRIPPER
# This function loads the JSON file you downloaded from the SpecRipper app.
def load_chunks(filepath):
    """Loads the chunk data from the specified JSON file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # We use pandas to easily handle the nested data
            df = pd.json_normalize(data, record_path=None)
            print(f"Successfully loaded {len(df)} chunks from {filepath}")
            return df
    except FileNotFoundError:
        print(f"Error: The file '{filepath}' was not found.")
        return None
    except json.JSONDecodeError:
        print(f"Error: The file '{filepath}' is not a valid JSON file.")
        return None

# 3. GENERATE SENTENCE EMBEDDINGS
# This function takes the text content of each chunk and converts it into a
# numerical vector (an "embedding") that represents its meaning.
def generate_embeddings(df, model):
    """Generates embeddings for the content of each chunk."""
    if df is None or 'content' not in df.columns:
        print("DataFrame is invalid or missing 'content' column.")
        return None
        
    print("Generating embeddings for each chunk... (This may take a moment)")
    # The model.encode method processes all the content at once for efficiency.
    embeddings = model.encode(df['content'].tolist(), convert_to_tensor=True, show_progress_bar=True)
    return embeddings

# 4. FIND AND FLAG REDUNDANT CHUNKS
# This is where the magic happens. We compare every chunk to every other chunk.
def find_redundant_chunks(df, embeddings, similarity_threshold=0.90):
    """
    Calculates cosine similarity between all embeddings and flags redundant chunks.
    A chunk is marked as redundant if it's highly similar to a preceding chunk.
    """
    if embeddings is None:
        print("Embeddings are not available. Skipping redundancy check.")
        return df

    print(f"Finding redundant chunks with a similarity threshold of {similarity_threshold}...")
    # Calculate cosine similarity between all pairs of embeddings
    cosine_scores = util.cos_sim(embeddings, embeddings)

    df['redundancy_info'] = [[] for _ in range(len(df))]
    df['is_redundant_of'] = None # New column to mark redundancy

    # Iterate through all pairs to check for similarity
    for i in range(len(cosine_scores)):
        for j in range(i + 1, len(cosine_scores)):
            score = cosine_scores[i][j].item()
            if score > similarity_threshold:
                # Get the chunk IDs
                chunk_id_i = df.loc[i, 'chunk_id']
                chunk_id_j = df.loc[j, 'chunk_id']
                
                print(f"Found high similarity ({score:.2f}) between {chunk_id_i} and {chunk_id_j}")

                # Add info to both chunks about the similarity
                df.loc[i, 'redundancy_info'].append({'related_chunk': chunk_id_j, 'score': score})
                df.loc[j, 'redundancy_info'].append({'related_chunk': chunk_id_i, 'score': score})
                
                # Mark the later chunk as a redundancy of the first one
                if df.loc[j, 'is_redundant_of'] is None:
                     df.loc[j, 'is_redundant_of'] = chunk_id_i

    return df

# 5. SAVE THE RESULTS
def save_results(df, output_filepath):
    """Saves the processed DataFrame back to a JSON file."""
    if df is None:
        print("No data to save.")
        return

    # Convert DataFrame back to the original nested JSON structure
    records = df.to_dict(orient='records')
    with open(output_filepath, 'w', encoding='utf-8') as f:
        json.dump(records, f, indent=2)
    print(f"Processed data saved to {output_filepath}")


# --- MAIN EXECUTION ---
if __name__ == "__main__":
    # Define your input and output file paths
    # IMPORTANT: Place the JSON file you downloaded from SpecRipper in this same folder
    # and make sure its name is `chunks.json`.
    input_file = "chunks.json"
    output_file = "chunks_processed.json"

    # Run the pipeline steps
    main_df = load_chunks(input_file)
    
    if main_df is not None:
        chunk_embeddings = generate_embeddings(main_df, model)
        processed_df = find_redundant_chunks(main_df, chunk_embeddings)
        save_results(processed_df, output_file)

```

## 4. How to Use the Pipeline

### Step 1: Get Your Data
- Use the SpecRipper web application (running inside Docker on your CentOS machine) to clean and chunk your PDF document.
- Click the **"Download Chunks"** button. This will save a `chunks.json` file to your CentOS machine's Downloads folder.

### Step 2: Prepare the Input
- Move the downloaded `chunks.json` file to the machine where you set up the Python environment (e.g., your Windows PC).
- Place it inside the `spec-pipeline` folder. The script expects it to be named exactly `chunks.json`.

### Step 3: Run the Script
- Make sure your Python virtual environment is still active (`(venv)` should be in your terminal prompt).
- Run the script from your terminal:
  ```bash
  python process.py
  ```

### Step 4: Review the Output
The script will run for a few moments. When it's finished, you will have a new file in your folder named `chunks_processed.json`. This file contains all the original data plus new information about which requirements are semantically similar or redundant. You can adapt this script to send the content to Llama 3 for requirement extraction.
